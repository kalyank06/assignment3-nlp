{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/Module-5-Assignment/blob/main/scripts/Part%20I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBtJv0BsJxNx"
   },
   "source": [
    "##Part I:  Vector Semantics and Motivation for Word Embeddings\n",
    "\n",
    "It is important to understand the words meaning (recall semantics) AND their context. Words that are seen in the similar context also often have similar meaning. The distributional hypothesis expresses this phenomenon saying that there is a link in similarity in how words are distributed and their likeness.  Vector semantics is the concept of learning representations of meanings of words – called embeddings—from their distributions in a corpus or corpora. Fundamentally, we are asking the question with NLP: how might we represent the meaning of a word and interpret it?\n",
    "\n",
    "A word embedding is simply a to represent words in a numerical context -- a vector.  This is important because Neural Networks and Machine Learning models don't learn on the text itself, but the numerical representation of the text. In fact, there is typically an \"embedding layer\" as part of the simplest NLP-based neural networks as you will find.\n",
    "\n",
    "The simplest way to show this is called a one-hot vector, other forms include term frequencies of words (as we have seen with Bayesian models), Term Frequency-Inverse Document Frequency, which normalizes terms across documents, and distributional representations, which are context-based encodings that help derive similarity-- i.e., \"queen is to female as king is to male\".\n",
    "\n",
    "We will start simple and discuss some of the challenges, then move to more complex transformations.\n",
    "\n",
    "## Setup\n",
    "As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input.\n",
    "\n",
    "It will be look like following:\n",
    "```\n",
    "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "'Some coding activity for you to complete'\n",
    "### END CODE HERE ###\n",
    "\n",
    "```\n",
    "Please be sure to fill these code snippets out as you turn in your assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/7b/0e/25d6b5678ed3c7e12bc94d047d0e9492e89cc78b7ea0034ac0f1cf2ff304/scikit_learn-1.4.1.post1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.4.1.post1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Downloading scikit_learn-1.4.1.post1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/10.6 MB 4.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/10.6 MB 4.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/10.6 MB 5.1 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/10.6 MB 4.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.8/10.6 MB 3.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/10.6 MB 3.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/10.6 MB 2.6 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.8/10.6 MB 2.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.9/10.6 MB 2.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.3/10.6 MB 2.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/10.6 MB 3.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.2/10.6 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.7/10.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/10.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.6/10.6 MB 4.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.2/10.6 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.9/10.6 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.5/10.6 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.0/10.6 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.7/10.6 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.4/10.6 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.1/10.6 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.6 MB 8.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.1/10.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.6/10.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 8.4 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.4.1.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKPWU-Ip2AZL"
   },
   "source": [
    "##1.1 One-hot vector\n",
    "\n",
    "A one-hot vector helps to translate categorical or sequential data to something that is machine readable and also does not have an impact on your model. Each word in the sequence is given a binary encoding and is mapped to a vector of the length of the the input. This is a common pre-processing step for the input layer in a neural network.\n",
    "\n",
    "One hot encoding assigns a unique code for each unique word. As an example, we can take the following sentence and convert it to a one-hot vector.\n",
    "\n",
    "\"Live as if you were to die tomorrow. Learn as if you were to live forever\"\n",
    "\n",
    "We will use NLTK to tokenize the sentence, then Sci-Kit Learn to apply the one-hot encoder. Note, that SK-Learn will apply a single value for a unique word in a vector which is great for categorical representations. This one-hot encoding has traditionally been used for feeding categorical data to many scikit-learn estimators in shallow learning models such as notably linear models and SVMs with the standard kernels.\n",
    "\n",
    "Note: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "id": "V5pWPsfD2BfJ",
    "outputId": "f71adb89-38e2-44bc-edf0-7ee4ceb16921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " One-Hot Encoded Vector using SKLearn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Koshike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>as</th>\n",
       "      <th>die</th>\n",
       "      <th>forever</th>\n",
       "      <th>if</th>\n",
       "      <th>learn</th>\n",
       "      <th>live</th>\n",
       "      <th>to</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>were</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     as  die forever   if learn live   to tomorrow were  you\n",
       "0   0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n",
       "1   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
       "2   0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
       "3   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n",
       "4   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n",
       "5   0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n",
       "6   0.0  1.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
       "7   0.0  0.0     0.0  0.0   0.0  0.0  0.0      1.0  0.0  0.0\n",
       "8   0.0  0.0     0.0  0.0   1.0  0.0  0.0      0.0  0.0  0.0\n",
       "9   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
       "10  0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
       "11  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n",
       "12  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n",
       "13  0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n",
       "14  0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n",
       "15  0.0  0.0     1.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from numpy import argmax\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "nltk.download('punkt')\n",
    "\n",
    "#%matplotlib inline\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# define input string\n",
    "data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n",
    "#tokenize that string\n",
    "wordlist = nltk.word_tokenize(data.lower())\n",
    "#create a vector representation of the wordlist\n",
    "wordlist_clean = []\n",
    "\n",
    "for i in wordlist: # Go through every word in your tokens list\n",
    "    if (i not in string.punctuation):  # remove punctuation\n",
    "        wordlist_clean.append(i)\n",
    "# define universe of possible input values\n",
    "wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n",
    "\n",
    "#encode using scki-kit learn\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(wordlist_clean_df)\n",
    "wordlist_clean_df_encoded = one_hot_encoder.transform(wordlist_clean_df)\n",
    "wordlist_clean_df_encoded = pd.DataFrame(data=wordlist_clean_df_encoded, columns=one_hot_encoder.categories_)\n",
    "print('\\n\\n One-Hot Encoded Vector using SKLearn')\n",
    "display(wordlist_clean_df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTRMr2m-oxRO"
   },
   "source": [
    "##1.2 Encoding as a dense  - Singular Value Decomposition\n",
    "A second approach you might try is to encode each word using a unique number. This helps with reducing dimensionality and attempts to address the problem of very large sparse matrices. Continuing the example above, you could assign 1 to \"live\", 2 to \"the\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector. Now, instead of a sparse vector, you now have a dense one. A dense vector is a vector where all elements are populated with a non-zero value.\n",
    "\n",
    "There are several challenges:\n",
    "\n",
    "1.   The integer-encoding is arbitrary (it does not capture any relationship between words)\n",
    "2.   An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
    "3.  Word order is ignored.\n",
    "4.  Raw absolute frequency counts of words do not necessarily represent the meaning of the text properly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "0C-2Kg9Dq05v",
    "outputId": "57b17d78-9f62-4edf-cde4-386fb0d680eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Koshike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>as</th>\n",
       "      <th>die</th>\n",
       "      <th>forever</th>\n",
       "      <th>if</th>\n",
       "      <th>learn</th>\n",
       "      <th>live</th>\n",
       "      <th>to</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>were</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as</td>\n",
       "      <td>die</td>\n",
       "      <td>forever</td>\n",
       "      <td>if</td>\n",
       "      <td>learn</td>\n",
       "      <td>live</td>\n",
       "      <td>to</td>\n",
       "      <td>tomorrow</td>\n",
       "      <td>were</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   as  die  forever  if  learn  live  to  tomorrow  were  you\n",
       "0  as  die  forever  if  learn  live  to  tomorrow  were  you\n",
       "1   2    1        1   2      1     2   2         1     2    2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from numpy import argmax\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "\"\"\"## Default Style Settings\n",
    "matplotlib.rcParams['figure.dpi'] = 150\n",
    "pd.options.display.max_colwidth = 200\n",
    "#%matplotlib inline\"\"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# define input string\n",
    "data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n",
    "#tokenize that string\n",
    "wordlist = nltk.word_tokenize(data.lower())\n",
    "#create a vector representation of the wordlist\n",
    "wordlist_clean = []\n",
    "\n",
    "for i in wordlist: # Go through every word in your tokens list\n",
    "    if (i not in string.punctuation):  # remove punctuation\n",
    "        wordlist_clean.append(i)\n",
    "# define universe of possible input values\n",
    "wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n",
    "dense_vector = np.unique(wordlist_clean_df, return_counts=True)\n",
    "dense_vector_df = pd.DataFrame(data=dense_vector, columns = np.unique(wordlist_clean_df))\n",
    "display(dense_vector_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5217etS2QW5"
   },
   "source": [
    "##1.3 Text Vectorization\n",
    "\n",
    "\n",
    "*   Overview\n",
    "*   N-Gram Bag of Words\n",
    "*   Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "*   Document Similarity: Cosine Similarity, Jaccard Similarity, Euclidian Similarity\n",
    "*   Topic Modeling Exercise\n",
    "\n",
    "### Why do we do it?\n",
    "These subsquent categories of text vectorizations are ways to derive similarity from text documents. This is useful for NLP tasks such as topic modeling -- where we aim to show the relationship between documents via a category or topic. You will see how TF-IDF can be used to support topic modeling.\n",
    "\n",
    "Here are some text vectorization approaches in summary:\n",
    "![Text Vectorization Approaches](https://drive.google.com/uc?export=view&id=12GYWDaK5_offSn3Gy-hv_KpuTc4A_mGA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKq4TXKo1zvi"
   },
   "source": [
    "\n",
    "### 1.3.1 N-Gram Bag-Of Words Model\n",
    "You've already learned the bag-of words model above with one-hot encoding and dense vectorization! We are counting the frequencies of words in the matrix in a dense representation of the word vector. What happens if we took some steps to improve the Bag-of-Words model by incorporating the n-gram approach we have learned earlier in the class.\n",
    "\n",
    "What does this do?\n",
    "If our goal is to identify words in texts that represent meaning of that text, then recall that taking the bi-gram, tri-gram, or n-gram of a corpus allows us to bring in context via the word order. With a simple BOW approach, no word order is considers. Moreover, we can filter words based on distributional counts -- that is, term frequencies. Imagine that the counts of a word fall into say a Gaussian (normal) Distribution across a number of corpora. We can use the distribution to filter out salient word-phrases or sequences in which we can infer the meaning of the text. Finally, we can apply weights to the frequency counts -- similar to weight vector in a NN-- in which those weights have an impact on word relationships or salience.\n",
    "\n",
    "\n",
    "![Bag of Words](https://drive.google.com/uc?export=view&id=1btCVz_8JWYTvE73qGLCRZb7nXg-kCiDU)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gR43ePKDKlu"
   },
   "source": [
    "#### 1.3.1.2 Example: N-Gram Bag-Of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "cv1H2Ha-0RMP",
    "outputId": "9861c66f-07ad-4762-fc0f-bdc7fbaa4ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The sky is blue and beautiful.' 'Love this blue and beautiful sky!'\n",
      " 'The quick brown fox jumps over the lazy dog.'\n",
      " \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\"\n",
      " 'I love green eggs, ham, sausages and bacon!'\n",
      " 'The brown fox is quick and the blue dog is lazy!'\n",
      " 'The sky is very blue and the sky is very beautiful today'\n",
      " 'The dog is lazy but the brown fox is quick!']\n",
      "==================================================\n",
      "['sky blue beautiful' 'love blue beautiful sky'\n",
      " 'quick brown fox jumps lazy dog'\n",
      " 'kings breakfast sausages ham bacon eggs toast beans'\n",
      " 'love green eggs ham sausages bacon' 'brown fox quick blue dog lazy'\n",
      " 'sky blue sky beautiful today' 'dog lazy brown fox quick']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Koshike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon eggs</th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>beautiful today</th>\n",
       "      <th>blue beautiful</th>\n",
       "      <th>blue dog</th>\n",
       "      <th>blue sky</th>\n",
       "      <th>breakfast sausages</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog lazy</th>\n",
       "      <th>eggs ham</th>\n",
       "      <th>...</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>love blue</th>\n",
       "      <th>love green</th>\n",
       "      <th>quick blue</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>sausages bacon</th>\n",
       "      <th>sausages ham</th>\n",
       "      <th>sky beautiful</th>\n",
       "      <th>sky blue</th>\n",
       "      <th>toast beans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon eggs  beautiful sky  beautiful today  blue beautiful  blue dog  \\\n",
       "0           0              0                0               1         0   \n",
       "1           0              1                0               1         0   \n",
       "2           0              0                0               0         0   \n",
       "3           1              0                0               0         0   \n",
       "4           0              0                0               0         0   \n",
       "5           0              0                0               0         1   \n",
       "6           0              0                1               0         0   \n",
       "7           0              0                0               0         0   \n",
       "\n",
       "   blue sky  breakfast sausages  brown fox  dog lazy  eggs ham  ...  lazy dog  \\\n",
       "0         0                   0          0         0         0  ...         0   \n",
       "1         0                   0          0         0         0  ...         0   \n",
       "2         0                   0          1         0         0  ...         1   \n",
       "3         0                   1          0         0         0  ...         0   \n",
       "4         0                   0          0         0         1  ...         0   \n",
       "5         0                   0          1         1         0  ...         0   \n",
       "6         1                   0          0         0         0  ...         0   \n",
       "7         0                   0          1         1         0  ...         0   \n",
       "\n",
       "   love blue  love green  quick blue  quick brown  sausages bacon  \\\n",
       "0          0           0           0            0               0   \n",
       "1          1           0           0            0               0   \n",
       "2          0           0           0            1               0   \n",
       "3          0           0           0            0               0   \n",
       "4          0           1           0            0               1   \n",
       "5          0           0           1            0               0   \n",
       "6          0           0           0            0               0   \n",
       "7          0           0           0            0               0   \n",
       "\n",
       "   sausages ham  sky beautiful  sky blue  toast beans  \n",
       "0             0              0         1            0  \n",
       "1             0              0         0            0  \n",
       "2             0              0         0            0  \n",
       "3             1              0         0            1  \n",
       "4             0              0         0            0  \n",
       "5             0              0         0            0  \n",
       "6             0              1         1            0  \n",
       "7             0              0         0            0  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example from: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/5-text-vectorization.html#\n",
    "\n",
    "import pandas as pd                        # Python library for pandas - data maniplation\n",
    "import numpy as np                         # Python library for numpy -- matrix algebra library\n",
    "import matplotlib                          # Python library for matplotlib -- visual display of data\n",
    "import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n",
    "import nltk                                # Python library for NLP\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "nltk.download('stopwords')                 # package for stop words\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Default Style Settings\n",
    "matplotlib.rcParams['figure.dpi'] = 150\n",
    "pd.options.display.max_colwidth = 200\n",
    "#%matplotlib inline\n",
    "\n",
    "corpus = [\n",
    "    'The sky is blue and beautiful.', 'Love this blue and beautiful sky!',\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "    'I love green eggs, ham, sausages and bacon!',\n",
    "    'The brown fox is quick and the blue dog is lazy!',\n",
    "    'The sky is very blue and the sky is very beautiful today',\n",
    "    'The dog is lazy but the brown fox is quick!'\n",
    "]\n",
    "labels = [\n",
    "    'weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather',\n",
    "    'animals'\n",
    "]\n",
    "\n",
    "corpus = np.array(corpus) # np.array better than list\n",
    "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "corpus_df\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "print(corpus)\n",
    "print(\"=\"*50)\n",
    "print(norm_corpus)\n",
    "\n",
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2, 2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names_out()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be0ML9kADimc"
   },
   "source": [
    "### 1.3.2 Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "As an extension of the BOW model, we can weight the frequency (counts) of the terms in a document by considering its *dispersion*. Fundamentally, we are taken the total frequency of a word and dividing it by the number of documents with that term -- this gives us term frequency.\n",
    "\n",
    "\n",
    "Then we take the inverse The formula for TF-IDF will look something like:\n",
    "\n",
    "\n",
    "*   Term Frequency(TF): the number of times a word appears in a document. These are the raw absolute frequency counts of the words in the BOW model.\n",
    "*   Inverse Document Frequency(IDF): total documents in corpus over number of documents with term.\n",
    "\n",
    "> $\\textit{TF-IDF} = {tf \\times idf}$\n",
    "\n",
    "Here, the general idea is that we can extropolate the meaningful words from a corpus by inversing their frequency. For example, \"The\" in the corpus may be frequently observed, but does not garner meaning. We can use this for keyword extraction, and information retrieval tasks.\n",
    "\n",
    "Let's normalize this function to account for divide-by-zero erros and to also smooth the weighting scheme.\n",
    "\n",
    "Addressing divide-by-zero errors. Similar to Laplace Smoothing techniques, we will typically add one to the IDF formula:\n",
    "\n",
    "> $\\textit{IDF} = 1 + log\\frac{N}{1+df}$\n",
    "\n",
    "We also might normalize the final IF-IDF function using an L2 Norm (see more in Jurafsky, Chapter 6).\n",
    "\n",
    "> $\\textit{TF-IDF}_{normalized} = \\frac{tf \\times idf}{\\sqrt{(tf\\times idf)^2}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4tJ1Yrbbufr"
   },
   "source": [
    "#### 1.3.2.1 Example: TF-IDF Usage\n",
    "In our example, we use the TfidfTransformer function to apply L2 norms and smoothing techniques.\n",
    "\n",
    "```\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "```\n",
    "\n",
    "\n",
    "Let's use the same corpus from above in our example for TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "6KROXdPWWRlv",
    "outputId": "08593b47-201f-469d-fa4e-39a5faa3c464"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_corpus = ['sky blue beautiful', 'love blue beautiful sky',\n",
    " 'quick brown fox jumps lazy dog',\n",
    " 'kings breakfast sausages ham bacon eggs toast beans',\n",
    " 'love green eggs ham sausages bacon', 'brown fox quick blue dog lazy',\n",
    " 'sky blue sky beautiful today' ,'dog lazy brown fox quick']\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "\"\"\"Note: With Tfidftransformer you will systematically compute word counts using CountVectorizer\n",
    "and then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores.\"\"\"\n",
    "tt = TfidfTransformer(norm='l2',\n",
    "                      use_idf=True,\n",
    "                      smooth_idf=True)\n",
    "tt_matrix = tt.fit_transform(cv_matrix)\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = cv.get_feature_names_out()\n",
    "tt_df = pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)\n",
    "display(tt_df)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Note: WWith Tfidfvectorizer on the contrary, you will do all three steps at once.\n",
    "Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0.,\n",
    "                     max_df=1.,\n",
    "                     norm='l2',\n",
    "                     use_idf=True,\n",
    "                     smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names_out()\n",
    "tv_df  = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "display(tv_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfXbJjDdV5V4"
   },
   "source": [
    "### 1.3.4 Document Similarity and Word Semantics\n",
    "\n",
    "Lexical semantics is a branch of linguistics focused on meaning and word relationships. Moreover, the idea behind word sense is the interpretation of the word (often requiring context to understand). Where multiple meanings can occur for a word – take the example of a mouse that can mean both the cursor controller and the rodent—we must discern using context. Relationships between word senses can be referred to as synonyms (e.g., couch/sofa).\n",
    "\n",
    "**Word Similarity** is not the same as a synonym, rather it is the idea that words have relationships. The example of cat and dog is used to show that while they are not synonymous, they are both animals, often they are domesticated – their semantics are similar.\n",
    "\n",
    "**Word relatedness** is slightly different than word similarity where there is more of a psychological association—for example, that coffee and cup are related.\n",
    "Recall that vectors for representing words are called embeddings implying that a point in space can be mapped to another point in space.\n",
    "\n",
    "This is important because word similarity (measured through a vector representing distance between two words in space) can be powerful for tasks we have previously done, such as sentiment analysis. Moreover, we can derive the meaning of the word using the nearby counts of similar words.\n",
    "\n",
    "We look at three similarity metrics to score word and/or document similarity:\n",
    "\n",
    "*   Manhattan Distance: is the sum of absolute differences between points across all the dimensions. Called \"Manhattan\" because we can think of getting from point (a,b) to point (c,d) on a Cartesian plane by only travelining vertically or horizontally, not diagnally.\n",
    "*   Euclidian Distance: is the shortest distance between two points in mathmatics. Not as useful in the field of NLP. The \"as the crow flies\" distance.\n",
    "*   Cosine Similarity: measure similarity based on the content overlap between documents.\n",
    "*   Jaccard Similarity: Used to identify documents we measure it as proportion of number of common words to number of unique words in both documents.\n",
    "\n",
    "Note: Generally speaking the difference betweem *distance* and *similarity* is basically that distance is just equal to 1 - similarity.\n",
    "\n",
    "\n",
    "Let's take a look at Cosine Similarity metrics since this is most commonly used with NLP and also with word2vec.\n",
    "\n",
    "> $similarity(doc_1, doc_2) = cos(\\theta) = \\frac{doc_1  doc_2}{\\lvert doc_1\\rvert \\lvert doc_2\\rvert}$\n",
    "\n",
    "By cosine distance/dissimilarity we assume following:\n",
    "> $distance(doc_1, doc_2) = 1 - similarity(doc_1, doc_2)$\n",
    "\n",
    "The similarity-based metics look like the following🇰\n",
    "> cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "cosine_similarity(xyz)\n",
    "array([[1.        , 0.97780241, 0.30320366],\n",
    "       [0.97780241, 1.        , 0.49613894],\n",
    "       [0.30320366, 0.49613894, 1.        ]])\n",
    "```\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1c9D33toCdC1W3_SRiUawykcspho8IVfI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBFAAHro4KyS"
   },
   "source": [
    "### **1.3.5: Exercise: BOW with n-gram**\n",
    "Use the *brown* corpus to create a n-gram BOW model. First, you must clean and organize the data. Then enter your code to complete the exercise.\n",
    "\n",
    "The Brown Corpus is an collection of text samples of American English categorized by various genres such as science-fiction, adventure, etc.\n",
    "\n",
    "Create a tri-gram bag of words matrix using the brown corpus as its inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wrrT-i-u4_7M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Koshike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Koshike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52700    His lawyer had sent him a statement on his overdue alimony , and there was a letter from the Collector of Internal Revenue asking him to stop in his office and explain last year's exemptions .\n",
      "19438                                                                                                        In this role of father confessor , he has always been most characteristic and most helpful .\n",
      "56419                                                                                                                                                    Mike felt disappointed that Mahmoud had left ; ;\n",
      "32385                                                                                                                       and this he could do only by going over its mass with the tracing procedure .\n",
      "16328                                                                                                                                            John recognized Ablard Corne and called out a greeting .\n",
      "                                                                                                       ...                                                                                               \n",
      "8121                                                                                                                                                                       Gross swung his swivel chair .\n",
      "29194                                                                                                                                           For the marksman , we study sets of five shots ( Af ) ; ;\n",
      "43239                                                                                                                                                                            No one saw Stacy leave .\n",
      "13651                                                                                                        This is our duty -- not as nurses or city employes -- but as citizens of the United States .\n",
      "47336                                                                                                                                `` The nuns there do a wonderful work '' , the President commented .\n",
      "Name: sentence, Length: 10000, dtype: object\n",
      "==================================================\n",
      "['lawyer sent statement overdue alimony letter collector internal revenue asking stop office explain last years exemptions'\n",
      " 'role father confessor always characteristic helpful'\n",
      " 'mike felt disappointed mahmoud left' ... 'one saw stacy leave'\n",
      " 'duty nurses city employes citizens united states'\n",
      " 'nuns wonderful work president commented']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aab follows vowel</th>\n",
       "      <th>aah go said</th>\n",
       "      <th>aaron cohn san</th>\n",
       "      <th>ab plane end</th>\n",
       "      <th>ab plasma control</th>\n",
       "      <th>abandon atom bomb</th>\n",
       "      <th>abandon ceremonies winter</th>\n",
       "      <th>abandon efforts would</th>\n",
       "      <th>abandon real celebration</th>\n",
       "      <th>abandon whole life</th>\n",
       "      <th>...</th>\n",
       "      <th>zion stayed get</th>\n",
       "      <th>zodiacal sign watercolorist</th>\n",
       "      <th>zoning board county</th>\n",
       "      <th>zoo grimace lions</th>\n",
       "      <th>zoo reservations extremely</th>\n",
       "      <th>zoooop snag around</th>\n",
       "      <th>zu longing simple</th>\n",
       "      <th>zur khaneh latter</th>\n",
       "      <th>zurich prince boun</th>\n",
       "      <th>zworykin novel technique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 72736 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aab follows vowel  aah go said  aaron cohn san  ab plane end  \\\n",
       "0                     0            0               0             0   \n",
       "1                     0            0               0             0   \n",
       "2                     0            0               0             0   \n",
       "3                     0            0               0             0   \n",
       "4                     0            0               0             0   \n",
       "...                 ...          ...             ...           ...   \n",
       "9995                  0            0               0             0   \n",
       "9996                  0            0               0             0   \n",
       "9997                  0            0               0             0   \n",
       "9998                  0            0               0             0   \n",
       "9999                  0            0               0             0   \n",
       "\n",
       "      ab plasma control  abandon atom bomb  abandon ceremonies winter  \\\n",
       "0                     0                  0                          0   \n",
       "1                     0                  0                          0   \n",
       "2                     0                  0                          0   \n",
       "3                     0                  0                          0   \n",
       "4                     0                  0                          0   \n",
       "...                 ...                ...                        ...   \n",
       "9995                  0                  0                          0   \n",
       "9996                  0                  0                          0   \n",
       "9997                  0                  0                          0   \n",
       "9998                  0                  0                          0   \n",
       "9999                  0                  0                          0   \n",
       "\n",
       "      abandon efforts would  abandon real celebration  abandon whole life  \\\n",
       "0                         0                         0                   0   \n",
       "1                         0                         0                   0   \n",
       "2                         0                         0                   0   \n",
       "3                         0                         0                   0   \n",
       "4                         0                         0                   0   \n",
       "...                     ...                       ...                 ...   \n",
       "9995                      0                         0                   0   \n",
       "9996                      0                         0                   0   \n",
       "9997                      0                         0                   0   \n",
       "9998                      0                         0                   0   \n",
       "9999                      0                         0                   0   \n",
       "\n",
       "      ...  zion stayed get  zodiacal sign watercolorist  zoning board county  \\\n",
       "0     ...                0                            0                    0   \n",
       "1     ...                0                            0                    0   \n",
       "2     ...                0                            0                    0   \n",
       "3     ...                0                            0                    0   \n",
       "4     ...                0                            0                    0   \n",
       "...   ...              ...                          ...                  ...   \n",
       "9995  ...                0                            0                    0   \n",
       "9996  ...                0                            0                    0   \n",
       "9997  ...                0                            0                    0   \n",
       "9998  ...                0                            0                    0   \n",
       "9999  ...                0                            0                    0   \n",
       "\n",
       "      zoo grimace lions  zoo reservations extremely  zoooop snag around  \\\n",
       "0                     0                           0                   0   \n",
       "1                     0                           0                   0   \n",
       "2                     0                           0                   0   \n",
       "3                     0                           0                   0   \n",
       "4                     0                           0                   0   \n",
       "...                 ...                         ...                 ...   \n",
       "9995                  0                           0                   0   \n",
       "9996                  0                           0                   0   \n",
       "9997                  0                           0                   0   \n",
       "9998                  0                           0                   0   \n",
       "9999                  0                           0                   0   \n",
       "\n",
       "      zu longing simple  zur khaneh latter  zurich prince boun  \\\n",
       "0                     0                  0                   0   \n",
       "1                     0                  0                   0   \n",
       "2                     0                  0                   0   \n",
       "3                     0                  0                   0   \n",
       "4                     0                  0                   0   \n",
       "...                 ...                ...                 ...   \n",
       "9995                  0                  0                   0   \n",
       "9996                  0                  0                   0   \n",
       "9997                  0                  0                   0   \n",
       "9998                  0                  0                   0   \n",
       "9999                  0                  0                   0   \n",
       "\n",
       "      zworykin novel technique  \n",
       "0                            0  \n",
       "1                            0  \n",
       "2                            0  \n",
       "3                            0  \n",
       "4                            0  \n",
       "...                        ...  \n",
       "9995                         0  \n",
       "9996                         0  \n",
       "9997                         0  \n",
       "9998                         0  \n",
       "9999                         0  \n",
       "\n",
       "[10000 rows x 72736 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd                        # Python library for pandas - data maniplation\n",
    "import numpy as np                         # Python library for numpy -- matrix algebra library\n",
    "import matplotlib                          # Python library for matplotlib -- visual display of data\n",
    "import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n",
    "import nltk                                # Python library for NLP\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "nltk.download('stopwords')                 # package for stop words\n",
    "nltk.download('brown')                 # package for stop words\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.corpus import brown              # this is the corpus you use for this exercise.\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#The seed() method is used to initialize the random number generator\n",
    "np.random.seed(100)\n",
    "\n",
    "brown_cat= brown.categories() # Creates a list of categories\n",
    "\n",
    "docs=[]\n",
    "for cat in brown_cat: # We append tuples of each document and categories in a list\n",
    "    t1=brown.sents(categories=cat) # At each iteration we retrieve all documents of a given category\n",
    "    for doc in t1:\n",
    "        docs.append((' '.join(doc), cat)) # These documents are appended as a tuple (document, category) in the list\n",
    "\n",
    "brown_df=pd.DataFrame(docs, columns=['sentence', 'category']) #The data frame is created using the generated tuple.\n",
    "\n",
    "brown_df.head()\n",
    "\n",
    "\n",
    "#Step 1. Pre-Processing the Brown Corpus Text\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "#create some normalized corpus from the pre-processing functiong above\n",
    "#normalize_corpus = normalize_corpus(docs)\n",
    "\n",
    "#Using the nromalized corpus.\n",
    "#Because the brown corpus is very large,select 10,000 random records from the corpus. Set seed so you can return the same results.\n",
    "norm_corpus = normalize_corpus(brown_df['sentence'].sample(n=10000,random_state=100))\n",
    "\n",
    "#Step 2. Create a tri-gram data frame and count its frequencies\n",
    "bv = CountVectorizer(ngram_range=(3, 3))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "\n",
    "\n",
    "print(brown_df['sentence'].sample(n=10000,random_state=100))\n",
    "print(\"=\"*50)\n",
    "print(norm_corpus)\n",
    "\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names_out() #get_feature_names_out\n",
    "\n",
    "#print the dataframe to show the tri-gram BOW\n",
    "pd.DataFrame(bv_matrix, columns=vocab)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1I-GlKzVk7B"
   },
   "source": [
    "### 1.3.6 Exercise: TD-IDF\n",
    "\n",
    "Now, using anyone of the following datasets, create you're own TF-IDF implementation. Provide your output in the form of a matrix.\n",
    "\n",
    "For more on the intuition behind TF-IDF, read the article [here](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html).\n",
    "\n",
    "Refer to [this article](https://sci2lab.github.io/ml_tutorial/tfidf/) related to TF-IDF and Elastisearch. Note how the TF-IDF approach can be used for information retrieval.\n",
    "\n",
    "Datasets that you may choose from:\n",
    "*   [Reviews Dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). this dataset uses classified data from Yelp!, Amazon, and IMBD. You can use this to determine TF-IDF across the datasets.\n",
    "\n",
    "\n",
    "*   Presidential speeches in NLTK. You can use this dataset to determine the TFIDF vector of words across presidential speeches.\n",
    "\n",
    "```\n",
    "nltk.corpus.inaugural\n",
    "```\n",
    "\n",
    "Please provide your code in the cell below.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "6Bckx5RzyUob"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\Koshike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                                                                                                         Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "1       Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order , and received on the 14th day of the...\n",
      "2       On the one hand , I was summoned by my Country , whose voice I can never hear but with veneration and love , from a retreat which I had chosen with the fondest predilection , and , in my flatterin...\n",
      "3       On the other hand , the magnitude and difficulty of the trust to which the voice of my country called me , being sufficient to awaken in the wisest and most experienced of her citizens a distrustf...\n",
      "4                          In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected .\n",
      "                                                                                                         ...                                                                                                   \n",
      "5215    May this be the story that guides us , the story that inspires us , and the story that tells ages yet to come that we answered the call of history , we met the moment ; democracy and hope , truth ...\n",
      "5216                                                                                                                            That is what we owe our forebearers , one another , and generations to follow .\n",
      "5217                      So with purpose and resolve we turn to those tasks of our time , sustained by faith , driven by conviction , and devoted to one another and the country we love with all our hearts .\n",
      "5218                                                                                                                                                   May God bless America , and may God protect our troops .\n",
      "5219                                                                                                                                                                                      Thank you , America .\n",
      "Name: sentence, Length: 5220, dtype: object\n",
      "==================================================\n",
      "['fellow citizens senate house representatives'\n",
      " 'among vicissitudes incident life event could filled greater anxieties notification transmitted order received th day present month'\n",
      " 'one hand summoned country whose voice never hear veneration love retreat chosen fondest predilection flattering hopes immutable decision asylum declining years retreat rendered every day necessary well dear addition habit inclination frequent interruptions health gradual waste committed time'\n",
      " ...\n",
      " 'purpose resolve turn tasks time sustained faith driven conviction devoted one another country love hearts'\n",
      " 'may god bless america may god protect troops' 'thank america']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abiding</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboriginal</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5220 rows × 2590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abandoned  abiding  abilities  ability  able  aboriginal  \\\n",
       "0         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "1         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "2         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "3         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "4         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "...       ...        ...      ...        ...      ...   ...         ...   \n",
       "5215      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5216      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5217      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5218      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5219      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "\n",
       "      abroad  absence  absolute  ...  year  years  yes   yet  yield  young  \\\n",
       "0        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "1        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "2        0.0      0.0       0.0  ...   0.0   0.14  0.0  0.00    0.0    0.0   \n",
       "3        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "4        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "...      ...      ...       ...  ...   ...    ...  ...   ...    ...    ...   \n",
       "5215     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.15    0.0    0.0   \n",
       "5216     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "5217     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "5218     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "5219     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "\n",
       "      youth  zeal  zealous  zealously  \n",
       "0       0.0   0.0      0.0        0.0  \n",
       "1       0.0   0.0      0.0        0.0  \n",
       "2       0.0   0.0      0.0        0.0  \n",
       "3       0.0   0.0      0.0        0.0  \n",
       "4       0.0   0.0      0.0        0.0  \n",
       "...     ...   ...      ...        ...  \n",
       "5215    0.0   0.0      0.0        0.0  \n",
       "5216    0.0   0.0      0.0        0.0  \n",
       "5217    0.0   0.0      0.0        0.0  \n",
       "5218    0.0   0.0      0.0        0.0  \n",
       "5219    0.0   0.0      0.0        0.0  \n",
       "\n",
       "[5220 rows x 2590 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abiding</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboriginal</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zealously</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5220 rows × 2590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abandoned  abiding  abilities  ability  able  aboriginal  \\\n",
       "0         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "1         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "2         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "3         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "4         0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "...       ...        ...      ...        ...      ...   ...         ...   \n",
       "5215      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5216      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5217      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5218      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "5219      0.0        0.0      0.0        0.0      0.0   0.0         0.0   \n",
       "\n",
       "      abroad  absence  absolute  ...  year  years  yes   yet  yield  young  \\\n",
       "0        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "1        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "2        0.0      0.0       0.0  ...   0.0   0.14  0.0  0.00    0.0    0.0   \n",
       "3        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "4        0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "...      ...      ...       ...  ...   ...    ...  ...   ...    ...    ...   \n",
       "5215     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.15    0.0    0.0   \n",
       "5216     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "5217     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "5218     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "5219     0.0      0.0       0.0  ...   0.0   0.00  0.0  0.00    0.0    0.0   \n",
       "\n",
       "      youth  zeal  zealous  zealously  \n",
       "0       0.0   0.0      0.0        0.0  \n",
       "1       0.0   0.0      0.0        0.0  \n",
       "2       0.0   0.0      0.0        0.0  \n",
       "3       0.0   0.0      0.0        0.0  \n",
       "4       0.0   0.0      0.0        0.0  \n",
       "...     ...   ...      ...        ...  \n",
       "5215    0.0   0.0      0.0        0.0  \n",
       "5216    0.0   0.0      0.0        0.0  \n",
       "5217    0.0   0.0      0.0        0.0  \n",
       "5218    0.0   0.0      0.0        0.0  \n",
       "5219    0.0   0.0      0.0        0.0  \n",
       "\n",
       "[5220 rows x 2590 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "nltk.download('inaugural')                 # package for stop words\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.corpus import inaugural              # this is the corpus you use for this exercise.\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "inaugural_files = inaugural.fileids()\n",
    "docs=[]\n",
    "for file in inaugural_files:\n",
    "    t1=inaugural.sents(fileids=file)\n",
    "    #print(t1)\n",
    "    for doc in t1:\n",
    "        docs.append((' '.join(doc), file))\n",
    "\n",
    "inaugural_df=pd.DataFrame(docs, columns=['sentence', 'yearOfPresident'])\n",
    "inaugural_df.head()\n",
    "\n",
    "#Step 1. Pre-Processing inaugural corpus\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens) #filtered_tokens\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(inaugural_df.sentence)\n",
    "\n",
    "print(inaugural_df.sentence)\n",
    "print(\"=\"*50)\n",
    "print(norm_corpus)\n",
    "\n",
    "# Step: 2 - TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=5,\n",
    "                     max_df=0.95)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix\n",
    "\n",
    "#Tfidftransformer\n",
    "tt = TfidfTransformer(norm='l2',\n",
    "                      use_idf=True,\n",
    "                      smooth_idf=True)\n",
    "tt_matrix = tt.fit_transform(cv_matrix)\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = cv.get_feature_names_out()\n",
    "tt_df = pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)\n",
    "display(tt_df)\n",
    "\n",
    "\"\"\"Note: WWith Tfidfvectorizer on the contrary, you will do all three steps at once.\n",
    "Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=5,\n",
    "                     max_df=0.95,\n",
    "                     norm='l2',\n",
    "                     use_idf=True,\n",
    "                     smooth_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names_out()\n",
    "tv_df  = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "display(tv_df)\n",
    "\n",
    "\n",
    "### END CODE HERE ###\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEGeTp9idVUk"
   },
   "source": [
    "##A. References\n",
    "\n",
    "1.   Chapter 6 – Vector Semantics and Word Embeddings Speech and Language rocessing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021.\n",
    "2.   [Word2vec from Scratch with NumPy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n",
    "3.   [A hands=on intutive approach to Deep Learning Methods for Text Data - Word2Vec,GloVe and FastText](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
    "4.    [Traditional Methods for Text Data](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n",
    "5.    [Word Embeddings](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb#scrollTo=Q6mJg1g3apaz)\n",
    "6. [CS 224D: Deep Learning for NLP](https://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf)\n",
    "7. [Text Vectorization] (https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html)\n",
    "8. [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
    "9. [TF-IDF](https://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)\n",
    "10. [Applying TF-IDF algorithm in practice](https://plumbr.io/blog/programming/applying-tf-idf-algorithm-in-practice)\n",
    "11. [text2vec](http://text2vec.org/similarity.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOyZQBiOWk75J27YoRK1g7z",
   "include_colab_link": true,
   "mount_file_id": "14hmD2J17784H_zxVA2HN9HEEuR6vt65C",
   "name": "M2_Assignment_Part_I.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
